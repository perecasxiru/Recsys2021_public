{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bf0845",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "This notebook sorts the data by timestamp.\n",
    "\n",
    "Data is expected to be located in `/data/recsys/part-XXXXX` and sorted data is going to be located in `/data/recsys/sorted/part-XXXXX.csv` where each file contains all the interactions of an hour.\n",
    "\n",
    "+ Min timestamp: 1612396800 (4 February 2021 0:00:00)\n",
    "+ Max timestamp: 1614211199 (24 February 2021 23:59:59) \n",
    "\n",
    "To convert from partID to hour do the following operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e9502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_to_timestamp(partID):\n",
    "    import pandas as pd\n",
    "    min_h = pd.to_datetime(partID * 3600 + 1612396800, unit='s')\n",
    "    max_h = pd.to_datetime((partID + 1) * 3600 + 1612396800 - 1, unit='s')\n",
    "\n",
    "    print(f'Part {partID:03d} from time {min_h} to {max_h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96beb5",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cecfd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 000 from time 2021-02-04 00:00:00 to 2021-02-04 00:59:59\n",
      "Part 100 from time 2021-02-08 04:00:00 to 2021-02-08 04:59:59\n",
      "Part 503 from time 2021-02-24 23:00:00 to 2021-02-24 23:59:59\n"
     ]
    }
   ],
   "source": [
    "part_to_timestamp(partID=0)\n",
    "part_to_timestamp(partID=100)\n",
    "part_to_timestamp(partID=503)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cd868",
   "metadata": {},
   "source": [
    "## Read and sort the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b677f",
   "metadata": {},
   "source": [
    "Sorting is separated in three different processes, otherwise it freezes the server. The three partitions are:\n",
    "+ Files from 0-99\n",
    "+ Files from 100-199\n",
    "+ Files from 200-290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "\n",
    "cluster = LocalCluster()\n",
    "# cluster.scale(3)\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type={\n",
    "              'bert': str,'hashtags':str,'tweet_id':str,'media':str,'links':str,'domains':str,'type':str,'language':str,'timestamp':np.uint32,\n",
    "              'AUTH_user_id':str,'AUTH_follower_count':np.uint32,'AUTH_following_count':np.uint32,'AUTH_verified':bool,'AUTH_account_creation':np.uint32,\n",
    "              'READ_user_id':str,'READ_follower_count':np.uint32,'READ_following_count':np.uint32,'READ_verified':bool,'READ_account_creation':np.uint32,\n",
    "              'auth_follows_read':bool,\n",
    "              'reply_timestamp':'Int32','retweet_timestamp':'Int32','quote_timestamp':'Int32','like_timestamp':'Int32'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647a5ae",
   "metadata": {},
   "source": [
    "First, we separate the data by hour. Since **1612396800** is the minimum timestamp we can do:\n",
    "$$\\text{part} = \\left[\\dfrac{\\text{timestamp} - 1612396800}{3600}\\right], \\ \\ \\text{with} \\ \\ [] \\ \\ \\text{the Floor function}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a8f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('/data/recsys/part-000*', names=list(column_type.keys()), header=None, sep='\\x01', dtype=column_type)\n",
    "df['part'] = df.apply(lambda row: (row.timestamp - 1612396800) // 3600, axis=1, meta=(None, 'uint16'))\n",
    "df.groupby('part').apply(lambda group: group.drop(columns=['part']).to_csv(f'/data/recsys/sorted/part-{group.name:05d}.csv',index=False), meta=df._meta).size.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('/data/recsys/part-001*', names=list(column_type.keys()), header=None, sep='\\x01', dtype=column_type)\n",
    "df['part'] = df.apply(lambda row: (row.timestamp-1612396800)//3600, axis=1, meta=(None, 'uint16'))\n",
    "df.groupby('part').apply(lambda group: group.drop(columns=['part']).to_csv(f'/data/recsys/sorted/part-{group.name:05d}.csv', mode='a', index=False, header=False), meta=df._meta).size.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('/data/recsys/part-002*', names=list(column_type.keys()), header=None, sep='\\x01', dtype=column_type)\n",
    "df['part'] = df.apply(lambda row: (row.timestamp-1612396800)//3600, axis=1, meta=(None, 'uint16'))\n",
    "df.groupby('part').apply(lambda group: group.drop(columns=['part']).to_csv(f'/data/recsys/sorted/part-{group.name:05d}.csv', mode='a', index=False, header=False), meta=df._meta).size.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b876",
   "metadata": {},
   "source": [
    "Then, we can sort files individually,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0034ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "for i in trange(0, 503):\n",
    "    df = pd.read_csv(f'/data/recsys/sorted/part-{i:05d}.csv')\n",
    "    df = df.sort_values('timestamp')\n",
    "    df.to_csv(f'/data/recsys/sorted/part-{i:05d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35567f8",
   "metadata": {},
   "source": [
    "### Train / validation\n",
    "\n",
    "Move files from `/data/recsys/sorted` folder into `/data/recsys/RecsysDocker/test` folder to replicate the twitter docker execution. <br>\n",
    "Files are sampled so the total is 10 millions approx.<br>\n",
    "Create the `reals.csv` to simulate the leaderboard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "721dd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /data/recsys/test_csv\n",
    "# !mkdir /data/recsys/RecsysDocker/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7fa3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "_from, _to = 504-24*7, 504\n",
    "_sample_each = int(1e7//(_to-_from)) # total ~ 10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa8386be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(_from, _to, leave=False):\n",
    "    f = f\"{i:05d}\"\n",
    "    !mv /data/recsys/sorted/part-{f}.csv /data/recsys/test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a39721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /data/recsys/test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f39120fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import trange\n",
    "reals = None\n",
    "for i in trange(_from, _to, leave=False):\n",
    "    df = pd.read_csv(f'/data/recsys/test_csv/part-{i:05d}.csv')\n",
    "    df = df.sample(_sample_each, replace=False)\n",
    "    if reals is None:\n",
    "        reals = df.iloc[:,-4:]\n",
    "    else:\n",
    "        reals = pd.concat([reals, df.iloc[:,-4:]])\n",
    "    df = df.iloc[:,:-4] \n",
    "    newi = i-_from\n",
    "    df.to_csv(f'/data/recsys/RecsysDocker/test/part-{newi:05d}', sep='\\x01', header=False, index=False)\n",
    "reals.notna().astype(int).to_csv(f'/data/recsys/RecsysDocker/reals.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
