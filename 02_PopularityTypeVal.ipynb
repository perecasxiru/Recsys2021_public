{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cc047b",
   "metadata": {},
   "source": [
    "# Popularity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7487e0",
   "metadata": {},
   "source": [
    "This notebook computes popularity feature for both reader and author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d1ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eef682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd593e94",
   "metadata": {},
   "source": [
    "## Reader popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27a4887-2c00-46f0-840c-4125f16c6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type = {\n",
    "    'bert': str, 'hashtags': str, 'tweet_id': str, 'media': str, 'links': str, 'domains': str, 'type': str, 'language': str, 'timestamp': np.uint32,\n",
    "    'AUTH_user_id':str,'AUTH_follower_count':np.uint32,'AUTH_following_count':np.uint32,'AUTH_verified':bool,'AUTH_account_creation':np.uint32,\n",
    "    'READ_user_id': str,'READ_follower_count':np.uint32,'READ_following_count':np.uint32,'READ_verified':bool,'READ_account_creation':np.uint32,\n",
    "    'auth_follows_read': bool,\n",
    "    'reply_timestamp':np.float32,'retweet_timestamp':np.float32,'quote_timestamp':np.float32,'like_timestamp':np.float32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805cbff6-d107-4958-9c8f-343a3da7c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "userid, userfo = 'AUTH', 'auth' #'AUTH', 'auth'  #'READ', 'read'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3f0ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cce8eff4af44fb297b78527cde55ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_merge=None\n",
    "df_none=True\n",
    "for idx in trange(504, 504+7*24):\n",
    "    \n",
    "    # Read a csv\n",
    "    df = pd.read_csv(f'/data/recsys/sorted/part-{idx:05d}.csv', \n",
    "                     usecols=[userid + '_user_id','type','reply_timestamp','retweet_timestamp','quote_timestamp','like_timestamp'], \n",
    "                     dtype=column_type)\n",
    "    \n",
    "    # Transform timestamp to integer (1 if timestamp present, 0 otherwise)\n",
    "    df['rep_T'] = ((df['reply_timestamp']>0) & (df['type']=='TopLevel')).astype(np.uint16)\n",
    "    df['ret_T'] = ((df['retweet_timestamp']>0) & (df['type']=='TopLevel')) .astype(np.uint16)\n",
    "    df['quo_T'] = ((df['quote_timestamp']>0) & (df['type']=='TopLevel')).astype(np.uint16)\n",
    "    df['lik_T'] = ((df['like_timestamp']>0) & (df['type']=='TopLevel')).astype(np.uint16)\n",
    "    \n",
    "    df['rep_R'] = ((df['reply_timestamp']>0) & (df['type']=='Retweet')).astype(np.uint16)\n",
    "    df['ret_R'] = ((df['retweet_timestamp']>0) & (df['type']=='Retweet')) .astype(np.uint16)\n",
    "    df['quo_R'] = ((df['quote_timestamp']>0) & (df['type']=='Retweet')).astype(np.uint16)\n",
    "    df['lik_R'] = ((df['like_timestamp']>0) & (df['type']=='Retweet')).astype(np.uint16)\n",
    "    \n",
    "    df['rep_Q'] = ((df['reply_timestamp']>0) & (df['type']=='Quote')).astype(np.uint16)\n",
    "    df['ret_Q'] = ((df['retweet_timestamp']>0) & (df['type']=='Quote')) .astype(np.uint16)\n",
    "    df['quo_Q'] = ((df['quote_timestamp']>0) & (df['type']=='Quote')).astype(np.uint16)\n",
    "    df['lik_Q'] = ((df['like_timestamp']>0) & (df['type']=='Quote')).astype(np.uint16)\n",
    "    \n",
    "    # One hot for the type\n",
    "    df['is_T'] = (df['type']=='TopLevel').astype(np.uint16)\n",
    "    df['is_R'] = (df['type']=='Retweet').astype(np.uint16)\n",
    "    df['is_Q'] = (df['type']=='Quote').astype(np.uint16)\n",
    "        \n",
    "    df = df.drop(columns=['type','reply_timestamp','retweet_timestamp','quote_timestamp','like_timestamp'])\n",
    "    \n",
    "    # Cumulated sum (interactions are sorted by timestamp)\n",
    "    df[['rep_cT', 'ret_cT', 'quo_cT', 'lik_cT', 'rep_cR', 'ret_cR', 'quo_cR', 'lik_cR', 'rep_cQ', 'ret_cQ', 'quo_cQ', 'lik_cQ', 'is_cT', 'is_cR', 'is_cQ']] = df.groupby(userid + '_user_id').cumsum().astype(np.uint16)\n",
    "    \n",
    "    if not df_none:\n",
    "        df = df.set_index(userid + '_user_id')\n",
    "        \n",
    "        # FIXED: It is important this line to have the reindex at the end, otherwise data will loose its initial order.\n",
    "        # Notebook 04.1 fixes that but here is already corrected\n",
    "        df = df.add(df_merge.loc[df_merge.index.intersection(df.index)].reindex(df.index), fill_value=0)\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    df['is_aT'] = df['is_cT'] - df['is_T']\n",
    "    df['is_aR'] = df['is_cR'] - df['is_R']\n",
    "    df['is_aQ'] = df['is_cQ'] - df['is_Q']\n",
    "    \n",
    "    # Compute cumulated mean\n",
    "    df['rep_aT'] = ((df['rep_cT'] - df['rep_T'])/df['is_aT']).fillna(0).astype(np.float16)\n",
    "    df['ret_aT'] = ((df['ret_cT'] - df['ret_T'])/df['is_aT']).fillna(0).astype(np.float16)\n",
    "    df['quo_aT'] = ((df['quo_cT'] - df['quo_T'])/df['is_aT']).fillna(0).astype(np.float16)\n",
    "    df['lik_aT'] = ((df['lik_cT'] - df['lik_T'])/df['is_aT']).fillna(0).astype(np.float16) \n",
    "    \n",
    "    df['rep_aR'] = ((df['rep_cR'] - df['rep_R'])/df['is_aR']).fillna(0).astype(np.float16)\n",
    "    df['ret_aR'] = ((df['ret_cR'] - df['ret_R'])/df['is_aR']).fillna(0).astype(np.float16)\n",
    "    df['quo_aR'] = ((df['quo_cR'] - df['quo_R'])/df['is_aR']).fillna(0).astype(np.float16)\n",
    "    df['lik_aR'] = ((df['lik_cR'] - df['lik_R'])/df['is_aR']).fillna(0).astype(np.float16) \n",
    "    \n",
    "    df['rep_aQ'] = ((df['rep_cQ'] - df['rep_Q'])/df['is_aQ']).fillna(0).astype(np.float16)\n",
    "    df['ret_aQ'] = ((df['ret_cQ'] - df['ret_Q'])/df['is_aQ']).fillna(0).astype(np.float16)\n",
    "    df['quo_aQ'] = ((df['quo_cQ'] - df['quo_Q'])/df['is_aQ']).fillna(0).astype(np.float16)\n",
    "    df['lik_aQ'] = ((df['lik_cQ'] - df['lik_Q'])/df['is_aQ']).fillna(0).astype(np.float16) \n",
    "    \n",
    "    \n",
    "        \n",
    "    # Get sum per user so we know the last value for the next parts\n",
    "    # Rename columns so values will be added to the cumulative ones\n",
    "    df_tmp = df.loc[:,[userid + '_user_id', 'rep_T', 'ret_T', 'quo_T', 'lik_T', 'rep_R', 'ret_R', 'quo_R', 'lik_R', 'rep_Q', 'ret_Q', 'quo_Q', 'lik_Q', 'is_T', 'is_R', 'is_Q']].groupby(userid + '_user_id').sum()    \n",
    "    df_tmp = df_tmp.rename(columns={'rep_T': 'rep_cT','ret_T':'ret_cT','quo_T':'quo_cT','lik_T':'lik_cT',\n",
    "                                    'rep_R': 'rep_cR','ret_R':'ret_cR','quo_R':'quo_cR','lik_R':'lik_cR',\n",
    "                                    'rep_Q': 'rep_cQ','ret_Q':'ret_cQ','quo_Q':'quo_cQ','lik_Q':'lik_cQ',\n",
    "                                    'is_T': 'is_cT', 'is_R': 'is_cR', 'is_Q':'is_cQ'})\n",
    "    \n",
    "    # Get the total count per user so we know the appearances of this user\n",
    "#     df_tmp[userid + '_count_user_id'] = df.groupby(userid + '_user_id').count()[userid + '_count_user_id']\n",
    "        \n",
    "    if df_none:\n",
    "        df_merge = df_tmp\n",
    "    else:\n",
    "        df_merge = df_merge.add(df_tmp, fill_value=0)\n",
    "        \n",
    "    df_none = False\n",
    "    \n",
    "    df[['rep_aT', 'ret_aT', 'quo_aT', 'lik_aT', 'rep_aR', 'ret_aR', 'quo_aR', 'lik_aR', 'rep_aQ', 'ret_aQ', 'quo_aQ', 'lik_aQ', 'is_aT', 'is_aR', 'is_aQ']].to_csv(f'/data/recsys/tef_{userfo}_tt/part-{idx:05d}.csv', index=False)\n",
    "    del df_tmp\n",
    "        \n",
    "    if (idx%50 == 0) and (idx > 0):\n",
    "        df_merge.to_csv(f'/data/recsys/tef_{userfo}_tt/{userfo}_merge4w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb2994a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv(f'/data/recsys/tef_{userfo}_tt/{userfo}_merge4w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc42da54-afe7-4c92-9187-25ab0e6fdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'/data/recsys/tef_{userfo}_tt/{userfo}_merge4w.csv')\n",
    "\n",
    "cols1 = ['rep_cT', 'ret_cT', 'quo_cT', 'lik_cT', 'rep_cR', 'ret_cR', 'quo_cR', 'lik_cR', 'rep_cQ', 'ret_cQ', 'quo_cQ', 'lik_cQ']\n",
    "cols2 = ['rep_aT', 'ret_aT', 'quo_aT', 'lik_aT', 'rep_aR', 'ret_aR', 'quo_aR', 'lik_aR', 'rep_aQ', 'ret_aQ', 'quo_aQ', 'lik_aQ']\n",
    "\n",
    "for c1, c2 in list(zip(cols1, cols2)):\n",
    "    df[c2] = df[c1]/df[f'is_c{c1[-1]}']\n",
    "    \n",
    "df.drop(columns = cols1).fillna(0).to_csv(f'/data/recsys/tef_{userfo}_tt/{userfo}_tt_merge4w_div.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
